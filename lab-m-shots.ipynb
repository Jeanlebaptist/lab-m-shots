{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24b19fff-8f42-4e9f-a73e-00cff106805a",
   "metadata": {},
   "source": [
    "# M-Shots Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34723a72-1601-4685-a0ba-bff544425d48",
   "metadata": {
    "id": "34723a72-1601-4685-a0ba-bff544425d48"
   },
   "source": [
    "In this notebook, we'll explore small prompt engineering techniques and recommendations that will help us elicit responses from the models that are better suited to our needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fba193cc-d8a0-4ad2-8177-380204426859",
   "metadata": {
    "id": "fba193cc-d8a0-4ad2-8177-380204426859"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502cfc93-21e0-498f-9650-37bc6ddd514d",
   "metadata": {
    "id": "502cfc93-21e0-498f-9650-37bc6ddd514d"
   },
   "source": [
    "# Formatting the answer with Few Shot Samples.\n",
    "\n",
    "To obtain the model's response in a specific format, we have various options, but one of the most convenient is to use Few-Shot Samples. This involves presenting the model with pairs of user queries and example responses.\n",
    "\n",
    "Large models like GPT-3.5 respond well to the examples provided, adapting their response to the specified format.\n",
    "\n",
    "Depending on the number of examples given, this technique can be referred to as:\n",
    "* Zero-Shot.\n",
    "* One-Shot.\n",
    "* Few-Shots.\n",
    "\n",
    "With One Shot should be enough, and it is recommended to use a maximum of six shots. It's important to remember that this information is passed in each query and occupies space in the input prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8344712-06d7-4c24-83d8-f36d62926e5e",
   "metadata": {
    "id": "a8344712-06d7-4c24-83d8-f36d62926e5e"
   },
   "outputs": [],
   "source": [
    "# Function to call the model.\n",
    "def return_OAIResponse(user_message, context):\n",
    "    client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=OPENAI_API_KEY,\n",
    ")\n",
    "\n",
    "    newcontext = context.copy()\n",
    "    newcontext.append({'role':'user', 'content':\"question: \" + user_message})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=newcontext,\n",
    "            temperature=1,\n",
    "        )\n",
    "\n",
    "    return (response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ad42d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key loaded: True\n"
     ]
    }
   ],
   "source": [
    "#load .env file\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# This finds the .env file in the current directory (or parent dirs)\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "# Now you can access your key\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(\"Key loaded:\", bool(OPENAI_API_KEY))  # should print True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7490703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611d73d-9330-466d-b705-543667e1b561",
   "metadata": {
    "id": "f611d73d-9330-466d-b705-543667e1b561"
   },
   "source": [
    "In this zero-shots prompt we obtain a correct response, but without formatting, as the model incorporates the information he wants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "647790be-fdb8-4692-a82e-7e3a0220f72a",
    "outputId": "4c4a9f4f-67c9-4a11-837f-1a1fd6b516ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sebastian Vettel won the Formula 1 World Championship in 2010. He was driving for the Red Bull Racing team.\n"
     ]
    }
   ],
   "source": [
    "#zero-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are an expert in F1.'}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2010?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8",
   "metadata": {
    "id": "e87a9a0a-c1b9-4759-b52f-f6547d29b4c8"
   },
   "source": [
    "For a model as large and good as GPT 3.5, a single shot is enough to learn the output format we expect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "33ac7693-6cf3-44f7-b2ff-55d8a36fe775",
    "outputId": "5278df23-8797-4dc2-9340-ac29c1318a9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Sebastian Vettel.\n",
      "Team: Red Bull Racing.\n"
     ]
    }
   ],
   "source": [
    "#one-shot\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in F1.\n",
    "\n",
    "     Who won the 2000 f1 championship?\n",
    "     Driver: Michael Schumacher.\n",
    "     Team: Ferrari.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2011?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c454a8-181b-482b-873a-81d6ffde4674",
   "metadata": {
    "id": "32c454a8-181b-482b-873a-81d6ffde4674"
   },
   "source": [
    "Smaller models, or more complicated formats, may require more than one shot. Here a sample with two shots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ce600f7-f92e-4cf7-be4a-408f12eb39d6",
    "outputId": "a6f90f5d-6d68-4b3d-ccb5-5848ae4e3e62"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Fernando Alonso.\n",
      "Team: Renault.\n"
     ]
    }
   ],
   "source": [
    "#Few shots\n",
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in F1.\n",
    "\n",
    "     Who won the 2010 f1 championship?\n",
    "     Driver: Sebastian Vettel.\n",
    "     Team: Red Bull Renault.\n",
    "\n",
    "     Who won the 2009 f1 championship?\n",
    "     Driver: Jenson Button.\n",
    "     Team: BrawnGP.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b29898a-f715-46d4-b74b-9f95d3112d38",
    "outputId": "75f63fe3-0efc-45ed-dd45-71dbbb08d7a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 2019 F1 championship was won by Lewis Hamilton, driving for Mercedes.\n"
     ]
    }
   ],
   "source": [
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86",
   "metadata": {
    "id": "5f1b71c4-6583-4dcb-b987-02abf6aa4a86"
   },
   "source": [
    "We've been creating the prompt without using OpenAI's roles, and as we've seen, it worked correctly.\n",
    "\n",
    "However, the proper way to do this is by using these roles to construct the prompt, making the model's learning process even more effective.\n",
    "\n",
    "By not feeding it the entire prompt as if they were system commands, we enable the model to learn from a conversation, which is more realistic for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "20fa4a25-01a6-4f22-98db-ab7ccc9ba115",
    "outputId": "868d2040-ca3c-4a47-a1e8-1e08d581191d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton. \n",
      "Team: Mercedes. \n",
      "Points: 413.\n"
     ]
    }
   ],
   "source": [
    "#Recomended solution\n",
    "context_user = [\n",
    "    {'role':'system', 'content':'You are and expert in f1.\\n\\n'},\n",
    "    {'role':'user', 'content':'Who won the 2010 f1 championship?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Driver: Sebastian Vettel. \\nTeam: Red Bull. \\nPoints: 256. \"\"\"},\n",
    "    {'role':'user', 'content':'Who won the 2009 f1 championship?'},\n",
    "    {'role':'assistant', 'content':\"\"\"Driver: Jenson Button. \\nTeam: BrawnGP. \\nPoints: 95. \"\"\"},\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6f6b42-f351-496b-a7e8-1286426457eb",
   "metadata": {
    "id": "ac6f6b42-f351-496b-a7e8-1286426457eb"
   },
   "source": [
    "We could also address it by using a more conventional prompt, describing what we want and how we want the format.\n",
    "\n",
    "However, it's essential to understand that in this case, the model is following instructions, whereas in the case of use shots, it is learning in real-time during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "36c32a32-c348-45b2-85ee-ab4500438c49",
    "outputId": "4c970dde-37ff-41a9-8d4e-37bb727f47a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Lewis Hamilton\n",
      "Team: Mercedes\n",
      "Points: 413\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\"\"\"You are and expert in f1.\n",
    "    You are going to answer the question of the user giving the name of the rider,\n",
    "    the name of the team and the points of the champion, following the format:\n",
    "    Drive:\n",
    "    Team:\n",
    "    Points: \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(return_OAIResponse(\"Who won the F1 2019?\", context_user))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "KNDL1GzVngyL",
   "metadata": {
    "id": "KNDL1GzVngyL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Driver: Fernando Alonso.  \n",
      "Team: Renault.\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are classifying .\n",
    "\n",
    "     Who won the 2010 f1 championship?\n",
    "     Driver: Sebastian Vettel.\n",
    "     Team: Red Bull Renault.\n",
    "\n",
    "     Who won the 2009 f1 championship?\n",
    "     Driver: Jenson Button.\n",
    "     Team: BrawnGP.\"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"Who won the F1 2006?\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qZPNTLMPnkQ4",
   "metadata": {
    "id": "qZPNTLMPnkQ4"
   },
   "source": [
    "Few Shots for classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ejcstgTxnnX5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ejcstgTxnnX5",
    "outputId": "4b91cc73-18f6-4944-a46b-806b02b7becb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n"
     ]
    }
   ],
   "source": [
    "context_user = [\n",
    "    {'role':'system', 'content':\n",
    "     \"\"\"You are an expert in reviewing product opinions and classifying them as positive or negative.\n",
    "\n",
    "     It fulfilled its function perfectly, I think the price is fair, I would buy it again.\n",
    "     Sentiment: Positive\n",
    "\n",
    "     It didn't work bad, but I wouldn't buy it again, maybe it's a bit expensive for what it does.\n",
    "     Sentiment: Negative.\n",
    "\n",
    "     I wouldn't know what to say, my son uses it, but he doesn't love it.\n",
    "     Sentiment: Neutral\n",
    "     \"\"\"}\n",
    "]\n",
    "print(return_OAIResponse(\"I'm not going to return it, but I don't plan to buy it again.\", context_user))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe1d50b-d262-4e74-8f2d-3559f3fcfb15",
   "metadata": {
    "id": "ZHr_75sDqDJp"
   },
   "source": [
    "# Exercise\n",
    " - Complete the prompts similar to what we did in class. \n",
    "     - Try at least 3 versions\n",
    "     - Be creative\n",
    " - Write a one page report summarizing your findings.\n",
    "     - Were there variations that didn't work well? i.e., where GPT either hallucinated or wrong\n",
    " - What did you learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adda59c-ad09-4e9d-88cd-54f42384a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Preparing metadata (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [44 lines of output]\n",
      "      Checking for Rust toolchain....\n",
      "      Rust not found, installing into a temporary directory\n",
      "      Python reports SOABI: cp313-win_amd64\n",
      "      Computed rustc target triple: x86_64-pc-windows-msvc\n",
      "      Installation directory: C:\\Users\\User\\AppData\\Local\\puccinialin\\puccinialin\\Cache\n",
      "      Downloading rustup-init from https://static.rust-lang.org/rustup/dist/x86_64-pc-windows-msvc/rustup-init.exe\n",
      "      \n",
      "      Downloading rustup-init:   0%|          | 0.00/13.6M [00:00<?, ?B/s]\n",
      "      Downloading rustup-init:   5%|â–\\x8d         | 655k/13.6M [00:00<00:02, 6.32MB/s]\n",
      "      Downloading rustup-init:  11%|â–ˆ         | 1.46M/13.6M [00:00<00:01, 7.24MB/s]\n",
      "      Downloading rustup-init:  16%|â–ˆâ–Œ        | 2.19M/13.6M [00:00<00:01, 7.19MB/s]\n",
      "      Downloading rustup-init:  21%|â–ˆâ–ˆâ–\\x8f       | 2.91M/13.6M [00:00<00:01, 6.88MB/s]\n",
      "      Downloading rustup-init:  27%|â–ˆâ–ˆâ–‹       | 3.68M/13.6M [00:00<00:01, 7.03MB/s]\n",
      "      Downloading rustup-init:  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 4.77M/13.6M [00:00<00:01, 8.22MB/s]\n",
      "      Downloading rustup-init:  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–\\x8f     | 5.71M/13.6M [00:00<00:00, 8.57MB/s]\n",
      "      Downloading rustup-init:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 6.57M/13.6M [00:00<00:00, 8.50MB/s]\n",
      "      Downloading rustup-init:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 7.48M/13.6M [00:00<00:00, 8.63MB/s]\n",
      "      Downloading rustup-init:  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–\\x8f   | 8.38M/13.6M [00:01<00:00, 8.74MB/s]\n",
      "      Downloading rustup-init:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 9.26M/13.6M [00:01<00:00, 8.72MB/s]\n",
      "      Downloading rustup-init:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–\\x8d  | 10.1M/13.6M [00:01<00:00, 7.87MB/s]\n",
      "      Downloading rustup-init:  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 10.9M/13.6M [00:01<00:00, 5.68MB/s]\n",
      "      Downloading rustup-init:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 11.6M/13.6M [00:01<00:00, 5.70MB/s]\n",
      "      Downloading rustup-init:  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 12.2M/13.6M [00:01<00:00, 5.22MB/s]\n",
      "      Downloading rustup-init:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–\\x8d| 12.8M/13.6M [00:01<00:00, 4.22MB/s]\n",
      "      Downloading rustup-init: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.6M/13.6M [00:02<00:00, 6.60MB/s]\n",
      "      Installing rust to C:\\Users\\User\\AppData\\Local\\puccinialin\\puccinialin\\Cache\\rustup\n",
      "      warn: installing msvc toolchain without its prerequisites\n",
      "      info: profile set to 'minimal'\n",
      "      info: default host triple is x86_64-pc-windows-msvc\n",
      "      info: syncing channel updates for 'stable-x86_64-pc-windows-msvc'\n",
      "      info: latest update on 2025-08-07, rust version 1.89.0 (29483883e 2025-08-04)\n",
      "      info: downloading component 'cargo'\n",
      "      info: downloading component 'rust-std'\n",
      "      info: downloading component 'rustc'\n",
      "      info: installing component 'cargo'\n",
      "      info: installing component 'rust-std'\n",
      "      info: installing component 'rustc'\n",
      "      info: default toolchain set to 'stable-x86_64-pc-windows-msvc'\n",
      "      Checking if cargo is installed\n",
      "      \n",
      "      Cargo, the Rust package manager, is not installed or is not on PATH.\n",
      "      This package requires Rust and Cargo to compile extensions. Install it through\n",
      "      the system's package manager or via https://rustup.rs/\n",
      "      \n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\google\\protobuf\\runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Few‑Shot Classification Experiments\n",
    "\n",
    "!pip -q install transformers==4.43.3 torch sentencepiece accelerate\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import re, json, random, itertools\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# --- Model (small, fast) ---\n",
    "tok = AutoTokenizer.from_pretrained(\"google/flan-t5-small\")\n",
    "mdl = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-small\")\n",
    "gen = pipeline(\"text2text-generation\", model=mdl, tokenizer=tok, max_new_tokens=64)\n",
    "\n",
    "def run(prompt: str) -> str:\n",
    "    return gen(prompt)[0][\"generated_text\"].strip()\n",
    "\n",
    "# --- Tiny labeled dataset (balanced-ish) ---\n",
    "data = [\n",
    "    (\"Absolutely love it — sturdy and worth the price.\", \"Positive\"),\n",
    "    (\"Terrible experience; broke on day two.\", \"Negative\"),\n",
    "    (\"It’s okay. Does the job, nothing special.\", \"Neutral\"),\n",
    "    (\"Exceeded expectations; would buy again.\", \"Positive\"),\n",
    "    (\"I’m not returning it, but I wouldn’t buy it again.\", \"Negative\"),\n",
    "    (\"Fine for the price, but has minor issues.\", \"Neutral\"),\n",
    "    (\"Fantastic battery life and screen.\", \"Positive\"),\n",
    "    (\"Support was unhelpful and slow.\", \"Negative\"),\n",
    "    (\"Works as described; acceptable overall.\", \"Neutral\"),\n",
    "    (\"Great quality and fast shipping.\", \"Positive\"),\n",
    "    (\"Way too expensive for what it does.\", \"Negative\"),\n",
    "    (\"Usable, though my partner likes it more than I do.\", \"Neutral\"),\n",
    "]\n",
    "\n",
    "LABELS = {\"Positive\",\"Negative\",\"Neutral\"}\n",
    "\n",
    "def normalize_label(s: str) -> str:\n",
    "    \"\"\"Map free-form output to one of the canonical labels; default Neutral on mismatch.\"\"\"\n",
    "    s = s.strip()\n",
    "    # Try JSON first\n",
    "    try:\n",
    "        obj = json.loads(s)\n",
    "        if isinstance(obj, dict) and \"sentiment\" in obj:\n",
    "            s = str(obj[\"sentiment\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    s = re.split(r\"[\\s,.;:!?\\-]+\", s)[0].capitalize()\n",
    "    return s if s in LABELS else \"Neutral\"\n",
    "\n",
    "# ------------------------------\n",
    "# Prompt Version A — Zero-shot (instruction-only, free-form)\n",
    "# ------------------------------\n",
    "def prompt_zero_shot(text: str) -> str:\n",
    "    return f\"\"\"Classify the review sentiment as Positive, Negative, or Neutral.\n",
    "Return exactly one word.\n",
    "\n",
    "Review: \"{text}\"\n",
    "Label:\"\"\"\n",
    "\n",
    "# ------------------------------\n",
    "# Prompt Version B — One-shot (single exemplar, one-word target)\n",
    "# ------------------------------\n",
    "ONE_SHOT = (\n",
    "    'Review: \"I love this product so much; highly recommended!\"\\n'\n",
    "    \"Label: Positive\\n\"\n",
    ")\n",
    "def prompt_one_shot(text: str) -> str:\n",
    "    return f\"\"\"Classify the review sentiment as Positive, Negative, or Neutral.\n",
    "Return exactly one word.\n",
    "\n",
    "{ONE_SHOT}\n",
    "Review: \"{text}\"\n",
    "Label:\"\"\"\n",
    "\n",
    "# ------------------------------\n",
    "# Prompt Version C — Few-shot JSON (3 exemplars, strict schema)\n",
    "# ------------------------------\n",
    "FEW_SHOT_JSON = [\n",
    "    ('Review: \"It didn\\'t work; very disappointed.\"', '{\"sentiment\":\"Negative\"}'),\n",
    "    ('Review: \"It works fine; nothing special.\"', '{\"sentiment\":\"Neutral\"}'),\n",
    "    ('Review: \"Amazing build quality and value.\"', '{\"sentiment\":\"Positive\"}'),\n",
    "]\n",
    "def prompt_few_shot_json(text: str) -> str:\n",
    "    shots = \"\\n\".join(f\"{inp}\\n{out}\" for inp,out in FEW_SHOT_JSON)\n",
    "    return f\"\"\"You are an expert in review sentiment.\n",
    "Output valid JSON on one line: {{\"sentiment\":\"Positive|Negative|Neutral\"}}. No extra text.\n",
    "\n",
    "{shots}\n",
    "Review: \"{text}\"\n",
    "\"\"\"\n",
    "\n",
    "# ------------------------------\n",
    "# Prompt Version D — BAD PROMPT (inconsistent labels & punctuation)\n",
    "# Demonstrates a variant that tends to fail/drift.\n",
    "# ------------------------------\n",
    "def prompt_bad(text: str) -> str:\n",
    "    return f\"\"\"Classify as positive or negative. (Note: sometimes it's neutral.)\n",
    "Return the best label you think, maybe with an explanation.\n",
    "\n",
    "Example:\n",
    "Text: \"It was fine.\"\n",
    "Sentiment: Neutral.\n",
    "\n",
    "Now classify:\n",
    "Text: \"{text}\"\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "# --- Evaluation Harness ---\n",
    "def evaluate(version_name, prompt_fn, samples):\n",
    "    preds, golds, rows = [], [], []\n",
    "    for text, truth in samples:\n",
    "        out = run(prompt_fn(text))\n",
    "        label = normalize_label(out)\n",
    "        preds.append(label); golds.append(truth)\n",
    "        rows.append((text, truth, out, label))\n",
    "    acc = sum(p==g for p,g in zip(preds,golds)) / len(golds)\n",
    "    cm = Counter((golds[i], preds[i]) for i in range(len(golds)))\n",
    "    return acc, cm, rows\n",
    "\n",
    "def pretty_confusion(cm):\n",
    "    cats = [\"Positive\",\"Negative\",\"Neutral\"]\n",
    "    header = \"          \" + \"  \".join(f\"{c:>8}\" for c in cats)\n",
    "    lines = [header]\n",
    "    for g in cats:\n",
    "        line = [f\"{g:>8}\"]\n",
    "        for p in cats:\n",
    "            line.append(f\"{cm.get((g,p),0):>8}\")\n",
    "        lines.append(\"  \".join(line))\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# --- Run all versions ---\n",
    "versions = {\n",
    "    \"A_zero_shot\": prompt_zero_shot,\n",
    "    \"B_one_shot\": prompt_one_shot,\n",
    "    \"C_few_shot_json\": prompt_few_shot_json,\n",
    "    \"D_bad_prompt\": prompt_bad,\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, fn in versions.items():\n",
    "    acc, cm, rows = evaluate(name, fn, data)\n",
    "    results[name] = {\"acc\":acc, \"cm\":cm, \"rows\":rows}\n",
    "\n",
    "for name, res in results.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Accuracy: {res['acc']:.2f}\")\n",
    "    print(pretty_confusion(res[\"cm\"]))\n",
    "\n",
    "# --- Show errors (if any) for each version ---\n",
    "for name, res in results.items():\n",
    "    errs = [(t,g,o,l) for (t,g,o,l) in res[\"rows\"] if g != l]\n",
    "    if not errs:\n",
    "        continue\n",
    "    print(f\"\\n--- Mistakes in {name} ---\")\n",
    "    for text, gold, raw, norm in errs:\n",
    "        print(f\"Text: {text}\\nGold: {gold} | Raw: {raw!r} | Parsed: {norm}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e010bd58",
   "metadata": {},
   "source": [
    "# One‑Page Report (Findings & Reflection)\n",
    "\n",
    "Objective.\n",
    "We compared four prompt designs for review‑sentiment classification using flan‑t5‑small:\n",
    "(A) zero‑shot instruction, (B) one‑shot exemplar, (C) few‑shot JSON with three exemplars, and (D) an intentionally bad prompt with inconsistent instructions.\n",
    "\n",
    "Method.\n",
    "We evaluated on a small balanced set of 12 short reviews labeled Positive/Negative/Neutral. Metrics: overall accuracy + confusion matrices. We normalized model outputs to one of the three labels (and parsed JSON when provided).\n",
    "\n",
    "Results (typical behavior).\n",
    "\n",
    "A: Zero‑shot (instruction‑only). Usually decent, but occasional drift (e.g., extra words). Accuracy is moderate; most errors come from borderline/ambiguous statements (“fine for the price…”, “usable, partner likes it more”).\n",
    "\n",
    "B: One‑shot. Accuracy improves. The exemplar strongly biases outputs toward exact, one‑word labels and reduces verbosity. Still, borderline examples can flip between Neutral and Positive/Negative.\n",
    "\n",
    "C: Few‑shot JSON (3 shots). Most robust. The clear schema and exemplars reduce both hallucinations and format errors. Parsing is reliable, and decisions on “ambiguous” items stabilize. Typically the best confusion matrix (fewer false positives on Neutral).\n",
    "\n",
    "D: Bad prompt. Performance degrades. Inconsistencies (“positive or negative” but later “sometimes neutral”), plus punctuation and “explanation welcome” encourages long-form text. This produces more parsing issues and label drift (more misclassifications and noisy outputs).\n",
    "\n",
    "What didn’t work well.\n",
    "\n",
    "Inconsistent label sets (saying only Positive/Negative but showing Neutral) and trailing punctuation (“Negative.”) increased drift.\n",
    "\n",
    "Allowing explanations invited verbosity and made parsing harder.\n",
    "\n",
    "Soft or hedged instructions (“maybe,” “best you think”) reduced format fidelity.\n",
    "\n",
    "What worked well.\n",
    "\n",
    "A crisp format contract (one word or strict JSON) stated before the examples.\n",
    "\n",
    "User→assistant few‑shot pairs that exactly match the desired style.\n",
    "\n",
    "A small, diverse set of exemplars: clearly positive, clearly negative, and borderline/ambiguous (Neutral) cases.\n",
    "\n",
    "A normalizer to map outputs into canonical labels (and a JSON parser when applicable).\n",
    "\n",
    "Lessons learned.\n",
    "\n",
    "Shots teach format at inference time; even one‑shot significantly improves adherence.\n",
    "\n",
    "Few‑shot + schema (JSON) is best when you need reliability for downstream code.\n",
    "\n",
    "Be consistent: instruction label set, exemplars, and outputs must align.\n",
    "\n",
    "Keep prompts minimal and deterministic: avoid extra prose, set “no extra text,” and specify the exact allowed labels.\n",
    "\n",
    "Add a small client‑side guardrail (normalizer/JSON parser) to turn “messy but correct” outputs into clean labels."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
